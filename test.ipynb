{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2024-04-05 05:41:09.071538: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-05 05:41:09.786252: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2, \"\n",
    "\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import diffusers\n",
    "import transformers\n",
    "\n",
    "from models.unet_2d_condition import UNet2DConditionModel\n",
    "from models.unet_3d import UNet3DConditionModel\n",
    "from models.mutual_self_attention import ReferenceAttentionControl\n",
    "from models.guidance_encoder import GuidanceEncoder\n",
    "from pipelines.pipeline_aggregation import MultiGuidance2LongVideoPipeline\n",
    "\n",
    "# diffusers==0.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint were not used when initializing UNet2DConditionModel: \n",
      " ['conv_norm_out.weight, conv_norm_out.bias, conv_out.weight, conv_out.bias']\n"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.load(\"./configs/inference.yaml\")\n",
    "\n",
    "vae = diffusers.AutoencoderKL.from_pretrained(cfg.vae_model_path)\n",
    "image_enc = transformers.CLIPVisionModelWithProjection.from_pretrained(cfg.image_encoder_path)\n",
    "\n",
    "denoising_unet = UNet3DConditionModel.from_pretrained_2d(\n",
    "    cfg.base_model_path,\n",
    "    cfg.motion_module_path,\n",
    "    subfolder=\"unet\",\n",
    "    unet_additional_kwargs=cfg.unet_additional_kwargs,\n",
    ")\n",
    "denoising_unet.load_state_dict(\n",
    "    torch.load(Path(cfg.ckpt_dir) / \"denoising_unet.pth\", map_location=\"cpu\"), \n",
    "    strict=False\n",
    ")\n",
    "denoising_unet.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "reference_unet = UNet2DConditionModel.from_pretrained(\n",
    "    cfg.base_model_path,\n",
    "    subfolder=\"unet\",\n",
    ")\n",
    "reference_unet.load_state_dict(\n",
    "    torch.load(Path(cfg.ckpt_dir) / \"reference_unet.pth\", map_location=\"cpu\"), \n",
    "    strict=False\n",
    ")\n",
    "reference_unet.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "guidance_encoder_group = {}\n",
    "for guidance_type in [\"depth\", \"normal\", \"semantic_map\", \"dwpose\"]:\n",
    "    guidance_encoder_module = GuidanceEncoder(\n",
    "        guidance_embedding_channels=cfg.guidance_encoder_kwargs.guidance_embedding_channels,\n",
    "        guidance_input_channels=cfg.guidance_encoder_kwargs.guidance_input_channels,\n",
    "        block_out_channels=cfg.guidance_encoder_kwargs.block_out_channels,\n",
    "    )\n",
    "    guidance_encoder_module.load_state_dict(\n",
    "        torch.load(Path(cfg.ckpt_dir) / f\"guidance_encoder_{guidance_type}.pth\", map_location=\"cpu\"),\n",
    "        strict=False,\n",
    "    )\n",
    "    guidance_encoder_group[f\"guidance_encoder_{guidance_type}\"] = guidance_encoder_module\n",
    "    \n",
    "\n",
    "sched_kwargs = OmegaConf.to_container(cfg.noise_scheduler_kwargs)\n",
    "if cfg.enable_zero_snr:\n",
    "    sched_kwargs.update( \n",
    "        rescale_betas_zero_snr=True,\n",
    "        timestep_spacing=\"trailing\",\n",
    "        prediction_type=\"v_prediction\",\n",
    "    )\n",
    "scheduler = diffusers.DDIMScheduler(**sched_kwargs)\n",
    "\n",
    "pipeline = MultiGuidance2LongVideoPipeline(\n",
    "    vae=vae,\n",
    "    image_encoder=image_enc,\n",
    "    reference_unet=reference_unet,\n",
    "    denoising_unet=denoising_unet,\n",
    "    **guidance_encoder_group,\n",
    "    scheduler=scheduler\n",
    ").to(\"cuda:0\", dtype=torch.float16)\n",
    "\n",
    "generator = torch.Generator(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 688\n"
     ]
    }
   ],
   "source": [
    "ref_image = Image.open(ref_image_path).convert(\"RGB\")\n",
    "ref_image_w, ref_image_h = ref_image.size\n",
    "target_w = 512\n",
    "target_h = int((ref_image_h / ref_image_w * target_w) // 8 * 8)\n",
    "print(target_w, target_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(768 - target_h) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image_path = Path(\"example_data/ref_images/test_output2.png\")\n",
    "motion_guidance_path = Path(\"/shared/3D-gen/priorMDM/behave-dataset/out/Date01_Sub01_monitor_hand\")\n",
    "seed = 1234\n",
    "# vid_len = 208\n",
    "\n",
    "ref_image = Image.open(ref_image_path).convert(\"RGB\")\n",
    "ref_image_w, ref_image_h = ref_image.size\n",
    "target_w = 512\n",
    "target_h = int((ref_image_h / ref_image_w * target_w) // 8 * 8)\n",
    "\n",
    "guidance_pil_group = dict()\n",
    "for guidance_type in ['depth', 'normal', 'semantic_map', 'dwpose', 'frames', 'mask']:\n",
    "    guidance_pil_group[guidance_type] = []\n",
    "    guidance_images = sorted(list(motion_guidance_path.glob(f\"{guidance_type}/*.png\")))\n",
    "    for guidance_image_path in guidance_images:\n",
    "        guidance_img = Image.open(guidance_image_path).convert(\"RGB\").resize((1024, 768))\n",
    "        if guidance_type == \"semantic_map\":\n",
    "            mask_path = Path(motion_guidance_path) / \"mask\" / guidance_image_path.name\n",
    "            mask_array = np.array(Image.open(mask_path).convert(\"RGB\").resize((1024, 768)))\n",
    "            guidance_img = Image.fromarray(np.where(mask_array > 0, np.array(guidance_img), 0))\n",
    "        guidance_img = guidance_img.crop((\n",
    "            (guidance_img.size[0] - target_w) // 2, \n",
    "            (guidance_img.size[1] - target_h) // 2, \n",
    "            (guidance_img.size[0] - target_w) // 2 + target_w, \n",
    "            (guidance_img.size[1] - target_h) // 2 + target_h, \n",
    "        ))\n",
    "        guidance_pil_group[guidance_type] += [guidance_img]\n",
    "\n",
    "video_length = len(list(guidance_pil_group.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e67abb2c2dc47f5927a1bc265fb1a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:02<00:00, 21.14it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    video_tensor = pipeline(\n",
    "        ref_image,\n",
    "        {k:v for k,v in guidance_pil_group.items() if k in ['depth', 'normal', 'semantic_map', 'dwpose']},\n",
    "        target_w, target_h,\n",
    "        video_length,\n",
    "        num_inference_steps=cfg.num_inference_steps,\n",
    "        guidance_scale=cfg.guidance_scale,\n",
    "        generator=generator.manual_seed(seed)\n",
    "    ).videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (3600, 692) to (3600, 704) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(\"test3.mp4\")\n",
    "\n",
    "B, C, video_length, H, W = video_tensor.shape\n",
    "video_tensor_reshaped = video_tensor.reshape(-1, C, H, W)\n",
    "resized_frames = F.interpolate(video_tensor_reshaped, size=(ref_image_h, ref_image_w), mode='bilinear', align_corners=False)\n",
    "resized_video = resized_frames.reshape(B, C, video_length, ref_image_h, ref_image_w).permute(2,0,1,3,4)\n",
    "\n",
    "writer = imageio.get_writer(output_path, fps=6)\n",
    "for i, x in enumerate(resized_video):\n",
    "    row = [x]\n",
    "    for k in list(guidance_pil_group.keys()):\n",
    "        im2 = guidance_pil_group[k][i]\n",
    "        im2 = torch.from_numpy(np.array(im2)) / 255.0\n",
    "        im2 = im2.permute(2,0,1).unsqueeze(0).to(x.device)\n",
    "        row.append(im2)\n",
    "    im2 = torch.cat(row)\n",
    "    x = make_grid(im2, nrow=7).permute(1,2,0)\n",
    "    x = (x * 255.).numpy().astype(np.uint8)\n",
    "    writer.append_data(x)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
